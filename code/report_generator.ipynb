{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "mnist_train = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('data', train=True, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                   torchvision.transforms.Lambda(lambda x: x.view(-1))\n",
    "                               ])),\n",
    "    batch_size=512, shuffle=True)\n",
    "\n",
    "mnist_test = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('data', train=False, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                   torchvision.transforms.Lambda(lambda x: x.view(-1))\n",
    "                               ])),\n",
    "    batch_size=512, shuffle=True)\n",
    "\n",
    "kmnist_train = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.KMNIST('data', train=True, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                   torchvision.transforms.Lambda(lambda x: x.view(-1))\n",
    "                               ])),\n",
    "    batch_size=512, shuffle=True)\n",
    "\n",
    "kmnist_test = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.KMNIST('data', train=False, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                    torchvision.transforms.ToTensor(),\n",
    "                                    torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                    torchvision.transforms.Lambda(lambda x: x.view(-1))\n",
    "                               ])),\n",
    "    batch_size=512, shuffle=True)\n",
    "\n",
    "fashion_train = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.FashionMNIST('data', train=True, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                   torchvision.transforms.Lambda(lambda x: x.view(-1))\n",
    "                               ])),\n",
    "    batch_size=512, shuffle=True)\n",
    "\n",
    "fashion_test = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.FashionMNIST('data', train=False, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                    torchvision.transforms.ToTensor(),\n",
    "                                    torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                    torchvision.transforms.Lambda(lambda x: x.view(-1))\n",
    "                               ])),\n",
    "    batch_size=512, shuffle=True)\n",
    "\n",
    "\n",
    "cifar10_train = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR10('data', train=True, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                   torchvision.transforms.Lambda(lambda x: x.view(-1))\n",
    "                               ])),\n",
    "    batch_size=512, shuffle=True)\n",
    "\n",
    "cifar10_test = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR10('data', train=False, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                    torchvision.transforms.ToTensor(),\n",
    "                                    torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                    torchvision.transforms.Lambda(lambda x: x.view(-1))\n",
    "                               ])),\n",
    "    batch_size=512, shuffle=True)\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'mnist': (mnist_train, mnist_test, 28*28),\n",
    "    'kmnist': (kmnist_train, kmnist_test, 28*28),\n",
    "    'fashion': (fashion_train, fashion_test, 28*28),\n",
    "    'cifar10': (cifar10_train, cifar10_test, 32*32*3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOTS_PER_ROW = 4\n",
    "\n",
    "ONLY_PDF = True\n",
    "CREATE_PDF = True\n",
    "TEST_MODELS = False\n",
    "\n",
    "USE_UMAP = False\n",
    "USE_FILTER_TDA = False\n",
    "\n",
    "DATASET = \"mnist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ff_mod.goodness import L2_Goodness, L1_Goodness, Norm_goodness, L2_Goodness_SQRT\n",
    "from ff_mod.probability import SigmoidProbability, SymmetricFFAProbability\n",
    "\n",
    "from ff_mod.network.base_ffa import FFANetwork, FFALayer \n",
    "\n",
    "from ff_mod.overlay import AppendToEndOverlay, CornerOverlay\n",
    "from ff_mod.loss import BCELoss\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "DIM = 1000\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "activations = {\n",
    "    'ReLU': torch.nn.ReLU(),\n",
    "    'Sigmoid': torch.nn.Sigmoid(),\n",
    "    'Tanh': torch.nn.Tanh()\n",
    "}\n",
    "\n",
    "goodnesses = {\n",
    "    'L2M_TK15': L2_Goodness(use_mean=True, topk_units=15),\n",
    "    'L2M': L2_Goodness(use_mean=True),\n",
    "    'L1M': L1_Goodness(use_mean=True),\n",
    "    'L1M_TK15': L1_Goodness(use_mean=True, topk_units=15),\n",
    "    'L2M_Split': L2_Goodness(positive_split=DIM//2, use_mean=True),\n",
    "    'L2M_TK15_Split': L2_Goodness(positive_split=DIM//2, use_mean=True, topk_units=15),\n",
    "    'L1M_Split': L1_Goodness(positive_split=DIM//2, use_mean=True),\n",
    "    'L1M_TK15_Split': L1_Goodness(positive_split=DIM//2, use_mean=True, topk_units=15),\n",
    "    'L2S': L2_Goodness(use_mean=False),\n",
    "    'L2S_TK15': L2_Goodness(use_mean=False, topk_units=15),\n",
    "    'L1S': L1_Goodness(use_mean=False),\n",
    "    'L1S_TK15': L1_Goodness(use_mean=False, topk_units=15),\n",
    "    'L2S_Split': L2_Goodness(positive_split=DIM//2, use_mean=False),\n",
    "    'L2S_TK15_Split': L2_Goodness(positive_split=DIM//2, use_mean=False, topk_units=15),\n",
    "    'L1S_Split': L1_Goodness(positive_split=DIM//2, use_mean=False),\n",
    "    'L1S_TK15_Split': L1_Goodness(positive_split=DIM//2, use_mean=False, topk_units=15),\n",
    "    'L2Sq' : L2_Goodness_SQRT(use_mean=False),\n",
    "    'L2Sq_TK15' : L2_Goodness_SQRT(use_mean=False, topk_units=15),\n",
    "    'L2Sq_Split' : L2_Goodness_SQRT(positive_split=DIM//2, use_mean=False),\n",
    "    'L2Sq_TK15_Split' : L2_Goodness_SQRT(positive_split=DIM//2, use_mean=False, topk_units=15),\n",
    "    'L2Mq' :  L2_Goodness_SQRT(use_mean=True),\n",
    "    'L2Mq_TK15' :  L2_Goodness_SQRT(use_mean=True, topk_units=15),\n",
    "    'L2Mq_Split' :  L2_Goodness_SQRT(positive_split=DIM//2, use_mean=True),\n",
    "    'L2Mq_TK15_Split' :  L2_Goodness_SQRT(positive_split=DIM//2, use_mean=True, topk_units=15)\n",
    "}\n",
    "\n",
    "goodness_map = {\n",
    "    \"L2_Goodness\": \"L2\",\n",
    "    \"L1_Goodness\": \"L1\",\n",
    "    \"L2_Goodness_Positive_Split_250\": \"L2_Split\",\n",
    "    \"L1_Goodness_Positive_Split_250\": \"L1_Split\",\n",
    "}\n",
    "\n",
    "probabilities = {\n",
    "    'SigmoidProbability_Theta_0': SigmoidProbability(theta=0),\n",
    "    'SigmoidProbability_Theta_2': SigmoidProbability(theta=2),\n",
    "    'SymmetricFFAProbability': SymmetricFFAProbability()\n",
    "}\n",
    "\n",
    "def create_network(goodness, activation, probability, input_size=784):\n",
    "    overlay = AppendToEndOverlay(pattern_size=100, num_classes=NUM_CLASSES, p=0.1)\n",
    "\n",
    "    network = FFANetwork(overlay)\n",
    "    \n",
    "    loss = BCELoss(probability_function=probabilities[probability])\n",
    "    \n",
    "    network.add_layer(FFALayer(input_size + 100, DIM, goodnesses[goodness], loss, activations[activation], learning_rate=LEARNING_RATE))\n",
    "    network.add_layer(FFALayer(DIM, DIM, goodnesses[goodness], loss, activations[activation], learning_rate=LEARNING_RATE))\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ff_mod.trainer import Trainer\n",
    "\n",
    "trainer = Trainer()\n",
    "trainer.set_dataloader(datasets[DATASET][0], datasets[DATASET][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total models: 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Traverse all folders and create networks\n",
    "import os\n",
    "import json \n",
    "\n",
    "MIN_ACCURATION = 0.00\n",
    "\n",
    "EXPERIMENT_FOLDER = f'experiments_train/{DATASET}/'\n",
    "\n",
    "USE_GOODNESS_MAP = False\n",
    "\n",
    "current_models = {}\n",
    "accuracies = {}\n",
    "\n",
    "\n",
    "\n",
    "for i, folder in tqdm(enumerate(os.listdir(EXPERIMENT_FOLDER)), leave=False):\n",
    "    \n",
    "    # Read json file config in folder\n",
    "    with open(os.path.join(EXPERIMENT_FOLDER, folder, 'config.json'), 'r') as f:\n",
    "        config = json.load(f)\n",
    "        network = create_network(config['goodness'], config['activation'], config['probability'], input_size=datasets[DATASET][2])\n",
    "        network.load_network(EXPERIMENT_FOLDER+'/' + folder + '/best_model')\n",
    "\n",
    "        if USE_GOODNESS_MAP:\n",
    "            config_str = f\"{config['activation']}_{goodness_map[config['goodness']]}_{config['probability']}\"\n",
    "        else:\n",
    "            config_str = f\"{config['activation']}_{config['goodness']}_{config['probability']}\"\n",
    "                \n",
    "        if config_str in current_models:\n",
    "            act_ind = 1\n",
    "            while config_str + f\"_{act_ind}\" in current_models:\n",
    "                act_ind += 1\n",
    "            config_str = config_str + f\"_{act_ind}\"\n",
    "        \n",
    "        if TEST_MODELS:\n",
    "            trainer.set_network(network)\n",
    "            acc = trainer.test_epoch(verbose=0)\n",
    "            \n",
    "            print(f\"{config_str} - {acc}\")\n",
    "            \n",
    "            if acc > MIN_ACCURATION:\n",
    "                current_models[config_str] = network\n",
    "                accuracies[config_str] = acc\n",
    "        else:\n",
    "            current_models[config_str] = network\n",
    "            \n",
    "print(f\"Total models: {len(current_models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_over_mean(latents, batch_size = 512, class_size = 10, dim = 1000):   \n",
    "\n",
    "    total_batches = latents.shape[0] // (batch_size * class_size)\n",
    "    \n",
    "    for batch in range(total_batches):\n",
    "        skip = batch * batch_size * class_size\n",
    "        for i in range(batch_size):\n",
    "            mean_state = np.zeros((dim))\n",
    "            \n",
    "            for j in range(class_size):\n",
    "                #print(f\"Spahes: {latents[skip + i + j * batch_size].shape} - {mean_state.shape}\")\n",
    "                mean_state += latents[skip + i + j * batch_size]\n",
    "            \n",
    "            mean_state /= class_size\n",
    "            \n",
    "            for j in range(10):\n",
    "                latents[skip + i + j * 512] -= mean_state\n",
    "    \n",
    "    return latents\n",
    "\n",
    "def get_latents(network, total_batches, layer = 1, use_train = True, normalize = False, remove_zeros = False, normalize_mean = False):\n",
    "    global trainer\n",
    "    \n",
    "    latents = []\n",
    "    labels = []\n",
    "    positiveness = []\n",
    "    \n",
    "    loader = trainer.train_loader if use_train else trainer.test_loader\n",
    "    \n",
    "    for i, (data, target) in enumerate(loader):\n",
    "        data, target = data.to(trainer.device), target.to(trainer.device)\n",
    "        if i >= total_batches:\n",
    "            break\n",
    "        \n",
    "        for l in range(10):\n",
    "            latent_t = network.get_latent(data, (target+l)%10, layer)\n",
    "            target_t = target.clone().detach()\n",
    "            \n",
    "            if remove_zeros:\n",
    "                target_t = target[torch.norm(latent_t, dim=1) > 0.01]\n",
    "                latent_t = latent_t[torch.norm(latent_t, dim=1) > 0.01]\n",
    "            \n",
    "            \n",
    "            \n",
    "            if normalize:\n",
    "                latent_t = latent_t / (torch.norm(latent_t, dim=1, keepdim=True) + 0.00001)\n",
    "            \n",
    "            latents.append(latent_t.detach().cpu().numpy())\n",
    "            labels.append(target_t.detach().cpu().numpy() * np.ones(latent_t.shape[0]))\n",
    "            \n",
    "            if l == 0:\n",
    "                positiveness.append(np.ones(latent_t.shape[0]))\n",
    "            else:\n",
    "                positiveness.append(np.zeros(latent_t.shape[0]))\n",
    "        \n",
    "    latents = np.concatenate(latents)\n",
    "    labels = np.concatenate(labels)\n",
    "    positiveness = np.concatenate(positiveness)\n",
    "    \n",
    "    if normalize_mean:\n",
    "        latent_t = normalize_over_mean(latents)\n",
    "        \n",
    "    return latents, labels, positiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_latents(all_models, use_train = True, normalize_mean = False):\n",
    "    all_latents = {}\n",
    "    \n",
    "    for model in all_models.keys():\n",
    "        remove_zeros = \"SymmetricFFAProbability\" in model\n",
    "        \n",
    "        all_latents[model] = get_latents(all_models[model], 2, use_train = use_train, remove_zeros=remove_zeros, normalize_mean=normalize_mean)\n",
    "        \n",
    "        if \"SymmetricFFAProbability\" in model:\n",
    "            all_latents[model + \"_Normalized\"] = get_latents(current_models[model], 2, use_train = use_train, normalize = True, remove_zeros=remove_zeros)\n",
    "            \n",
    "    return all_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ONLY_PDF:\n",
    "    all_latents = get_all_latents(current_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "\n",
    "def get_tsne(all_latents, verbose = 1, limit = 4000, plot = False, plot_class = False, plot_possetive = False, use_umap = False):\n",
    "    \"\"\" Recieves a dictionary of latents and plots them in matplotlib in subplots\"\"\"\n",
    "    \n",
    "    total_plots = len(all_latents.keys())\n",
    "    \n",
    "    assert total_plots > 0\n",
    "    \n",
    "    if limit is None:\n",
    "        for model in all_latents.keys():\n",
    "            if all_latents[model][0].shape[0] > 4000:\n",
    "                raise ValueError(f'Latent {model} is too big with {all_latents[model][0].shape[0]} samples.')\n",
    "    \n",
    "    if plot or plot_class or plot_possetive:\n",
    "        if total_plots > PLOTS_PER_ROW:\n",
    "            rows = math.ceil(total_plots / PLOTS_PER_ROW)\n",
    "            fig, ax = plt.subplots(rows, PLOTS_PER_ROW, figsize=(5 * PLOTS_PER_ROW, 5 * rows))\n",
    "        else:\n",
    "            fig, ax = plt.subplots(1, total_plots, figsize=(5* total_plots, 5))\n",
    "    \n",
    "    fig.suptitle('TSNE of latents')\n",
    "    \n",
    "    tsne_latents = {}\n",
    "    \n",
    "    for i, model in tqdm(enumerate(all_latents.keys()), leave=False, total=total_plots):\n",
    "        latents, labels, positiveness = all_latents[model]\n",
    "\n",
    "        # Random samples\n",
    "        if limit is not None and latents.shape[0] > limit:\n",
    "            rand_indices = np.random.choice(latents.shape[0], limit, replace=False)\n",
    "        else:\n",
    "            rand_indices = np.arange(latents.shape[0])\n",
    "        \n",
    "        if not use_umap:\n",
    "            tsne = TSNE(n_components=2, verbose=verbose, n_iter=700)\n",
    "        else:\n",
    "            tsne = umap.UMAP(n_components=2, verbose=verbose)\n",
    "\n",
    "        tsne_results = tsne.fit_transform(latents[rand_indices])\n",
    "        \n",
    "        tsne_latents[model] = tsne_results\n",
    "        \n",
    "        if total_plots > PLOTS_PER_ROW:\n",
    "            if plot_class:\n",
    "                ax[i//PLOTS_PER_ROW, i%PLOTS_PER_ROW].scatter(tsne_results[:,0], tsne_results[:,1], c=labels[rand_indices], cmap='Set3', s = 1)\n",
    "                ax[i//PLOTS_PER_ROW, i%PLOTS_PER_ROW].set_title(model)\n",
    "            elif plot_possetive:\n",
    "                ax[i//PLOTS_PER_ROW, i%PLOTS_PER_ROW].scatter(tsne_results[:,0], tsne_results[:,1], c=positiveness[rand_indices], cmap='bwr', s = 1)\n",
    "                ax[i//PLOTS_PER_ROW, i%PLOTS_PER_ROW].set_title(model)\n",
    "            elif plot:\n",
    "                ax[i//PLOTS_PER_ROW, i%PLOTS_PER_ROW].scatter(tsne_results[:,0], tsne_results[:,1], alpha=0.5)\n",
    "                ax[i//PLOTS_PER_ROW, i%PLOTS_PER_ROW].set_title(model)\n",
    "        else:\n",
    "            if plot_class:\n",
    "                ax[i].scatter(tsne_results[:,0], tsne_results[:,1], c=labels[rand_indices], cmap='Set3', s = 1)\n",
    "                ax[i].set_title(model)\n",
    "            elif plot_possetive:\n",
    "                ax[i].scatter(tsne_results[:,0], tsne_results[:,1], c=positiveness[rand_indices], cmap='bwr', s = 1)\n",
    "                ax[i].set_title(model)\n",
    "            elif plot:\n",
    "                ax[i].scatter(tsne_results[:,0], tsne_results[:,1], alpha=0.5)\n",
    "                ax[i].set_title(model)\n",
    "\n",
    "    return tsne_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ONLY_PDF:\n",
    "    _ = get_tsne(all_latents, limit = 1800, verbose=0, plot_possetive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latents(all_latents, amount = 1000, range = (400, 600)):\n",
    "    total_plots = len(all_latents.keys())\n",
    "    \n",
    "    if len(all_latents.keys()) > PLOTS_PER_ROW:\n",
    "        rows = math.ceil(total_plots / PLOTS_PER_ROW)\n",
    "        fig, ax = plt.subplots(rows, PLOTS_PER_ROW, figsize=(5 * PLOTS_PER_ROW, 5 * rows))\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, len(all_latents.keys()), figsize=(5* len(all_latents.keys()), 5))\n",
    "    \n",
    "    \n",
    "    fig.suptitle('Plot of latents')\n",
    "    \n",
    "    for i, model in enumerate(all_latents.keys()):\n",
    "        latents, labels, positiveness = all_latents[model]\n",
    "        \n",
    "        #random_indices = np.random.choice(latents.shape[0], amount, replace=False)\n",
    "        normal_latents = latents / (np.linalg.norm(latents, axis=1)[:, None]+0.000001)\n",
    "        if len(all_latents.keys()) > PLOTS_PER_ROW:\n",
    "            ax[i//PLOTS_PER_ROW, i%PLOTS_PER_ROW].imshow(normal_latents[:amount, range[0]:range[1]], cmap='gray', aspect='auto')\n",
    "            ax[i//PLOTS_PER_ROW, i%PLOTS_PER_ROW].set_title(model)\n",
    "        else:\n",
    "            ax[i].imshow(normal_latents[:amount, range[0]:range[1]], cmap='gray', aspect='auto')\n",
    "            ax[i].set_title(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ONLY_PDF:\n",
    "    plot_latents(all_latents, amount = 1000, range=(0, DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_positives(all_latents, eps = 0.05):\n",
    "    total_plots = len(all_latents.keys())\n",
    "    \n",
    "    mean_positives = {}\n",
    "    \n",
    "    if len(all_latents.keys()) > PLOTS_PER_ROW:\n",
    "        rows = rows = math.ceil(total_plots / PLOTS_PER_ROW)\n",
    "        fig, ax = plt.subplots(rows, PLOTS_PER_ROW, figsize=(5 * PLOTS_PER_ROW, 5 * rows))\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, len(all_latents.keys()), figsize=(5* len(all_latents.keys()), 5))\n",
    "    \n",
    "    fig.suptitle('Use of neurons in latents')\n",
    "    \n",
    "    for i, model in enumerate(all_latents.keys()):\n",
    "        latents, labels, positiveness = all_latents[model]\n",
    "        \n",
    "        pos_counts = np.sum(latents[positiveness == 1] > eps, axis=1)\n",
    "        neg_counts = np.sum(latents[positiveness == 0] > eps, axis=1)\n",
    "        \n",
    "        mean_positives[model] = pos_counts.mean() + neg_counts.mean() * 1/9\n",
    "        \n",
    "        max_bins = min(max(np.max(pos_counts), np.max(neg_counts), 2), 20)\n",
    "        \n",
    "        if len(all_latents.keys()) > PLOTS_PER_ROW:\n",
    "            ax[i//PLOTS_PER_ROW, i%PLOTS_PER_ROW].hist(neg_counts, bins=max_bins, color='b', alpha=0.7, weights=np.ones(len(neg_counts))/len(neg_counts))\n",
    "            ax[i//PLOTS_PER_ROW, i%PLOTS_PER_ROW].hist(pos_counts, bins=max_bins, color='r', alpha=0.7, weights=np.ones(len(pos_counts))/len(pos_counts))\n",
    "            \n",
    "            ax[i//PLOTS_PER_ROW, i%PLOTS_PER_ROW].set_title(model)\n",
    "        else:\n",
    "            ax[i].hist(neg_counts, bins=max_bins, color='b', alpha=0.7, weights=np.ones(len(neg_counts))/len(neg_counts))\n",
    "            ax[i].hist(pos_counts, bins=max_bins, color='r', alpha=0.7, weights=np.ones(len(pos_counts))/len(pos_counts))\n",
    "            \n",
    "            ax[i].set_title(model)\n",
    "    \n",
    "    return mean_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ONLY_PDF:\n",
    "    mean_positives = plot_positives(all_latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hoyer_distribution(all_latents, eps = 1e-6):\n",
    "    hoyer_mean = {}\n",
    "    \n",
    "    for i, model in enumerate(all_latents.keys()):\n",
    "        latents, labels, positiveness = all_latents[model]\n",
    "        \n",
    "        hoyer = np.linalg.norm(latents, ord=1, axis=1) / (np.linalg.norm(latents, ord=2, axis=1)+eps)\n",
    "        hoyer = (np.sqrt(DIM) - hoyer) / (np.sqrt(DIM) - 1)\n",
    "        \n",
    "        hoyer_mean[model] = hoyer.mean()\n",
    "    \n",
    "    return hoyer_mean\n",
    "\n",
    "def plot_hoyer_distribution(all_latents, eps = 1e-6):\n",
    "    total_plots = len(all_latents.keys())\n",
    "    \n",
    "    hoyer_mean = {}\n",
    "    \n",
    "    if len(all_latents.keys()) > PLOTS_PER_ROW:\n",
    "        rows = rows = math.ceil(total_plots / PLOTS_PER_ROW)\n",
    "        fig, ax = plt.subplots(rows, PLOTS_PER_ROW, figsize=(5 * PLOTS_PER_ROW, 5 * rows))\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, len(all_latents.keys()), figsize=(5* len(all_latents.keys()), 5))\n",
    "    \n",
    "    fig.suptitle('Hoyer Sparsity of latents')\n",
    "    \n",
    "    for i, model in enumerate(all_latents.keys()):\n",
    "        latents, labels, positiveness = all_latents[model]\n",
    "        \n",
    "        hoyer = np.linalg.norm(latents, ord=1, axis=1) / (np.linalg.norm(latents, ord=2, axis=1)+eps)\n",
    "        \n",
    "        hoyer = (np.sqrt(DIM) - hoyer) / (np.sqrt(DIM) - 1)\n",
    "        \n",
    "        hoyer_mean[model] = hoyer.mean()\n",
    "        \n",
    "        if len(all_latents.keys()) > PLOTS_PER_ROW:\n",
    "            ax[i//PLOTS_PER_ROW, i%PLOTS_PER_ROW].hist(hoyer, bins=50)\n",
    "            ax[i//PLOTS_PER_ROW, i%PLOTS_PER_ROW].set_title(model)\n",
    "            ax[i//PLOTS_PER_ROW, i%PLOTS_PER_ROW].set_xlim(0, 1)\n",
    "        else:\n",
    "            ax[i].hist(hoyer, bins=50)\n",
    "            ax[i].set_title(model)\n",
    "            ax[i].set_xlim(0, 1)\n",
    "    \n",
    "    return hoyer_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ONLY_PDF:\n",
    "    hoyer_mean = get_hoyer_distribution(all_latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ONLY_PDF:\n",
    "    plot_positives(all_latents, eps = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def get_best_eps(latents, limit = 1000, percentage = 0.9, steps = 10):\n",
    "    # Do a binary search to find the best eps\n",
    "    latents = latents[:limit]\n",
    "    \n",
    "    best_eps = 0\n",
    "    min_eps, max_eps = 0.01, np.max(cdist(latents, latents))\n",
    "    \n",
    "    for i in range(steps):\n",
    "        eps = (min_eps + max_eps) / 2\n",
    "        db = DBSCAN(eps=eps, min_samples=7).fit(latents)\n",
    "        \n",
    "        if np.sum(db.labels_ != -1) / limit < percentage:\n",
    "            min_eps = eps\n",
    "        else:\n",
    "            max_eps = eps\n",
    "    \n",
    "    best_eps = (min_eps + max_eps) / 2\n",
    "    \n",
    "    return best_eps\n",
    "    \n",
    "\n",
    "def get_filtered_data(all_latents, min_samples = 7, eps = 2, percentage = 0.9, batch_size = 3000):\n",
    "    filtered_latents = {}\n",
    "    \n",
    "    for i, model in tqdm(enumerate(all_latents.keys()), leave=False):\n",
    "        latents, labels, positiveness = all_latents[model]\n",
    "        \n",
    "        best_eps = get_best_eps(latents, percentage = percentage)\n",
    "        \n",
    "        # Since the dataset may be to big, we will use a batched approach\n",
    "        latents_result = []\n",
    "        labels_result = []\n",
    "        positiveness_result = []\n",
    "        \n",
    "        for i in range(0, len(latents), batch_size):\n",
    "            db = DBSCAN(eps=best_eps, min_samples=min_samples).fit(latents[i:i+batch_size])\n",
    "            \n",
    "            latents_result.append(latents[i:i+batch_size][db.labels_ != -1])\n",
    "            labels_result.append(labels[i:i+batch_size][db.labels_ != -1])\n",
    "            positiveness_result.append(positiveness[i:i+batch_size][db.labels_ != -1])\n",
    "            \n",
    "        latents = np.concatenate(latents_result)\n",
    "        labels = np.concatenate(labels_result)\n",
    "        positiveness = np.concatenate(positiveness_result)\n",
    "        \n",
    "        filtered_latents[model] = (latents, labels, positiveness)\n",
    "    \n",
    "    return filtered_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ripser import ripser\n",
    "from persim import plot_diagrams\n",
    "from ripser import Rips\n",
    "\n",
    "# Required for a LaTeX error\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "\n",
    "def get_ph_diagrams(all_latents, max_dim = 1, verbose = True, limit = 1700):\n",
    "    \n",
    "    total_plots = len(all_latents.keys())\n",
    "    \n",
    "    assert total_plots > 0\n",
    "    \n",
    "    rips = Rips(maxdim=max_dim, verbose=verbose)\n",
    "    \n",
    "    if total_plots > PLOTS_PER_ROW:\n",
    "        rows = math.ceil(total_plots / PLOTS_PER_ROW)\n",
    "        \n",
    "        fig, ax = plt.subplots(2 * rows, PLOTS_PER_ROW, figsize=(5 * PLOTS_PER_ROW, 2 * 5 * rows), gridspec_kw={'hspace': 0.2})\n",
    "    else:\n",
    "        fig, ax = plt.subplots(2, len(all_latents.keys()), figsize=(5 * total_plots, 2 * 5), gridspec_kw={'hspace': 0.2})\n",
    "    \n",
    "    fig.suptitle('pi 0 of latents')\n",
    "    \n",
    "    diagrams_coords = {}\n",
    "    \n",
    "    for i, model in tqdm(enumerate(all_latents.keys()), leave=False, total=total_plots):\n",
    "        if limit is not None and all_latents[model][0].shape[0] > limit:\n",
    "            rand_indices = np.random.choice(all_latents[model][0].shape[0], limit, replace=False)\n",
    "        else:\n",
    "            rand_indices = np.arange(all_latents[model][0].shape[0])\n",
    "            \n",
    "        diagrams = rips.fit_transform(all_latents[model][0][rand_indices])\n",
    "        \n",
    "        if total_plots > PLOTS_PER_ROW:\n",
    "            plot_diagrams(diagrams, ax=ax[2 * (i // PLOTS_PER_ROW), i % PLOTS_PER_ROW], lifetime=True, size=5, title=model)\n",
    "            ax[2 * (i // PLOTS_PER_ROW) + 1, i % PLOTS_PER_ROW].hist(diagrams[0][:, 1][:-1], bins=100)\n",
    "        else:\n",
    "            plot_diagrams(diagrams, ax=ax[0, i], lifetime=True, size=5, title=model)\n",
    "            ax[1,i].hist(diagrams[0][:, 1][:-1], bins=100)\n",
    "\n",
    "        diagrams_coords[model] = diagrams\n",
    "        \n",
    "    return diagrams_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ONLY_PDF:\n",
    "    _ = get_ph_diagrams(all_latents, max_dim = 0, verbose = False, limit = 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.spatial.distance import cdist\n",
    "def compute_separability(all_latents, kfactor = 5, batch_size = 32, limit = 4000):\n",
    "    \"\"\" Compute the separability of the latents by using the separability index\"\"\"\n",
    "    \n",
    "    figure = plt.figure(figsize=(11.69,8.27))\n",
    "    figure.suptitle('Separability')\n",
    "    \n",
    "    separability = {}\n",
    "    \n",
    "    for i, model in enumerate(all_latents.keys()):\n",
    "        latents_pre, labels_pre, positiveness_pre = all_latents[model]\n",
    "        \n",
    "        random_indices = np.random.choice(latents_pre.shape[0], limit, replace=False)\n",
    "        latents, labels, positiveness = latents_pre[random_indices], labels_pre[random_indices], positiveness_pre[random_indices]\n",
    "        \n",
    "        pos_sum, neg_sum = 0, 0\n",
    "\n",
    "        for i in tqdm(range(math.ceil(latents.shape[0]/batch_size)), leave=False):\n",
    "            \n",
    "            # For each batch, compute the indexes of the kfactor nearest neighbors\n",
    "            indexes = np.argsort(cdist(latents[batch_size * i : batch_size * i + batch_size], latents), axis=1)[:, 1 : 1 + kfactor]\n",
    "            \n",
    "            current_positives = positiveness[batch_size * i : batch_size * i + batch_size] == 1\n",
    "            current_negatives = positiveness[batch_size * i : batch_size * i + batch_size] == 0\n",
    "            \n",
    "            pos_sum += np.sum(positiveness[indexes[current_positives]] == 1)\n",
    "            neg_sum += np.sum(positiveness[indexes[current_negatives]] == 0)\n",
    "            \n",
    "        total_pos = np.sum(positiveness == 1) * kfactor\n",
    "        total_neg = np.sum(positiveness == 0) * kfactor\n",
    "        \n",
    "        separability[model] = ((pos_sum / total_pos) + (neg_sum / total_neg))/2\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.table(cellText=[[k, v] for k, v in separability.items()], colLabels=[\"Model\", \"Separability\"], loc='center')\n",
    "    \n",
    "    return separability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ONLY_PDF:\n",
    "    separability_mean = compute_separability(all_latents, batch_size=128, limit=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a PDF with all of them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file \"report_mnist_3_bigtest.pdf\" created successfully.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Finished",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 59\u001b[0m\n\u001b[0;32m     55\u001b[0m create_report(filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreport_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_3_bigtest.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m#create_report(filename = f\"report_{DATASET}_train_3_final.pdf\", use_train = True)\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m#create_report(filename = f\"report_{DATASET}_train_meaned_3_final.pdf\", use_train = True, normalize_over_mean=True)\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: Finished"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "import os\n",
    "\n",
    "def create_report(filename = \"report.pdf\", use_train = False, normalize_over_mean = False):\n",
    "\n",
    "    all_latents = get_all_latents(current_models, use_train=use_train, normalize_mean=normalize_over_mean)\n",
    "\n",
    "    os.makedirs(f'reports/{DATASET}/', exist_ok=True)\n",
    "\n",
    "    with PdfPages(f'reports/{DATASET}/'+filename) as pdf:\n",
    "        # Create and save each plot in a new page\n",
    "        _ = get_tsne(all_latents, limit = 1500, verbose=0, plot_possetive=True)\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "        \n",
    "        if USE_UMAP:\n",
    "            _ = get_tsne(all_latents, limit = 1800, verbose=0, plot_possetive=True, use_umap=True)\n",
    "            pdf.savefig()\n",
    "            plt.close()\n",
    "        \n",
    "        _ = plot_latents(all_latents, amount = 512 * 2, range=(0, DIM))\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "        \n",
    "        plot_positives(all_latents)\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "        \n",
    "        plot_hoyer_distribution(all_latents)\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "\n",
    "        _ = get_ph_diagrams(all_latents, max_dim = 0, verbose = False, limit = 5000)\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "        \n",
    "        #_ = compute_separability(all_latents, batch_size=128*2, limit=512*10)\n",
    "        #pdf.savefig()\n",
    "        #plt.close()\n",
    "        \n",
    "        if USE_FILTER_TDA:\n",
    "            FILTER_PERCENTAGE = 0.95\n",
    "            all_filtered_latents = get_filtered_data(all_latents, percentage=FILTER_PERCENTAGE)\n",
    "            \n",
    "            _ = get_ph_diagrams(all_filtered_latents, max_dim = 0, verbose = False, limit = 6500)\n",
    "            plt.title(f\"Persistence Diagrams Filtered ({FILTER_PERCENTAGE})\")\n",
    "            pdf.savefig()\n",
    "            plt.close()\n",
    "        \n",
    "    print(f'PDF file \"{filename}\" created successfully.')\n",
    "\n",
    "if CREATE_PDF:\n",
    "    create_report(filename = f\"report_{DATASET}_3_bigtest.pdf\", use_train = False)\n",
    "    #create_report(filename = f\"report_{DATASET}_train_3_final.pdf\", use_train = True)\n",
    "    #create_report(filename = f\"report_{DATASET}_train_meaned_3_final.pdf\", use_train = True, normalize_over_mean=True)\n",
    "\n",
    "    raise Exception(\"Finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Forward",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
